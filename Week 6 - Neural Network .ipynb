{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Components of NN\n",
    "    1. Neuron\n",
    "    2. Wights/connections\n",
    "    3. Input layer \n",
    "    4. Hidden Layer \n",
    "    5. Ouput layer\n",
    "    6. Activation function\n",
    "    7. Weight Initialization\n",
    "    8. Batch size\n",
    "    9. Batch Normalization\n",
    "    10. Dropout\n",
    "    11. Back propgation\n",
    "    12. Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Neurons\n",
    "The building block for neural networks are (artificial) neurons. These are simple computational\n",
    "units that have weighted input signals and produce an output signal using an activation function.\n",
    "\n",
    "#### 2. Weights and bias\n",
    "Weights and bias are basically similiar concept of coefficient and intercept in the linear regression.Weights are often initialized to small random values, such as values in the range 0 to 0.3, although more complex initialization schemes can be used. Like linear regression, larger weights indicate increased complexity and fragility of the model. It is desirable to keep weights in the network small and regularization techniques can be used.\n",
    "\n",
    "#### 3. Input layer \n",
    "Any layer in the neural network is made up of neurons and connected to another layer by weights. Input layer is the first layer of a neural network, i.e all the X's(features) fed to this layer.\n",
    "\n",
    "#### 4. Hidden layer\n",
    "\n",
    "Hidden layers are intermediate layers b/w input and output layer. These layers are core of any neural network, responsible for learning representation on different levels of details.\n",
    "\n",
    "#### 5. Output layer\n",
    "\n",
    "Output layer is usually the last layer of a neural network. All the predicted values are neurons of the output layers. For instance, for an multi-class classification, the size of an output layer is simply the number of classes; 2 for a binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('StructureOfNN.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Can a single layer(hidden) neural network be considered as a linear/logistic regression? If not, can we derive any relationship bw neurel net and regression?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 6. Activation function\n",
    "An activation function is a simple mapping of summed weighted input to the output of the neuron. It is called an activation function because it governs the threshold at which the neuron is activated and the strength of the output signal.\n",
    "\n",
    "So, role of the activation function in a neural network is to produce a non-linear decision boundary via linear combinations of the weighted inputs.\n",
    "\n",
    "#### Why?\n",
    "If we do not apply a Activation function then the output signal would simply be a simple linear function.A linear function is just a polynomial of one degree. Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. A Neural Network without Activation function would simply be a Linear regression Model, which has limited power and does not performs good most of the times. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid\n",
    "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### activation function \n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return(1.0/(1.0+np.exp(-z)))\n",
    "x = np.array([-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return(((2.0/(1.0+np.exp(-2*z))) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, sigmoid(x))\n",
    "plt.plot(x, np.tanh(x), 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One point to mention is that the gradient is stronger for tanh than sigmoid ( derivatives are steeper). Deciding between the sigmoid or tanh will depend on your requirement of gradient strength. There are many other activation functions available, the following are most commenly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('activation_function_cheatsheet2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 7. Weight Initialization\n",
    "The weights for first step of the training are mostly randomnly initiated. Then updated on every step of the optimizer, A step is defined as processing a batch of datapoint through feedforward and backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializer(sizes):\n",
    "    num_layers = len(sizes)\n",
    "    biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "    weights = [np.random.randn(y, x)\n",
    "                    for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    return(num_layers, biases, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will initilize weights and biases form the normal distribution with mean 0 and variance 1. Let's try with an example of 3 layers with sizes [784, 30, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers, biases, weights = initializer([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(biases[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: As it's mentioned most of the time we inizialise the weights, but for the recent past there are different pre-trained saved weights for many deeplearning architectures. We can also use those weight as our initial weight and train the network for specific data we want to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 8. Batch size \n",
    "The weights are updated on every step of a learning algo (optimizer), updating weights based on large number datapoints eventually requires lot of memory. The batch size helps to handle this issue. Since you train network using less number of samples the overall training procedure requires less memory. It's especially important in case if you are not able to fit dataset in memory.\n",
    "\n",
    "#### 9. Batch Nomalization\n",
    "Normalize the activation of each previous layer at each batch with N(0,1). Allows higher learning rate, faster training. Act as regulization, simplifies the creation of deeper networks.\n",
    "\n",
    "#### 10. Dropout\n",
    "Drops a fraction of nuerons randomly for eaech iteration, helps as a regularization factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 11. Back propagation\n",
    "Backpropagation is about understanding how changing the weights and biases in a network changes the cost function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('backprop.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(x, y):\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer (z = Wx+b)\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    # backward pass\n",
    "    delta = cost_function(activations[-1], y) * \\\n",
    "        sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    # Note that the variable l in the loop below is used a little\n",
    "    # differently to the notation in Chapter 2 of the book.  Here,\n",
    "    # l = 1 means the last layer of neurons, l = 2 is the\n",
    "    # second-last layer, and so on.  It's a renumbering of the\n",
    "    # sigmoid_prime in the book, used here to take advantage of the fact\n",
    "    # that Python can use negative indices in lists.\n",
    "    for l in range(2, num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return(nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>12. Optimizer</b>\n",
    "\n",
    "The Process by wich we minimize the cost function - difference between Ground truth and the prediction\n",
    "\n",
    "Can you suggest some optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Alogrithm\n",
    "\n",
    "A neural network can be descibed in following steps:\n",
    "\n",
    "    1. initialize weights(W) and bias(b) for input x1, x2, x3, ...,xp\n",
    "\n",
    "    2. feed forward through the weights updated/initialized\n",
    "         2a. obtain Wx+b\n",
    "         2b. apply activation function\n",
    "\n",
    "    3. back propagate the error (compute gradients, update the weights)\n",
    "         3a. Calculate error\n",
    "         3b. Compute the gradients deltaW and deltab (the rate of change in errors repect to the weights and bias) \n",
    "         3c. Update the the weight with delta - w+deltaW ; b+delb\n",
    "\n",
    "    4. repeat the step 2 and 3 for n number of epochs\n",
    "\n",
    "Note: the feed forward goes through l no.of layers (1 to l)then errors calculated for each l and then backpropagation happen all the layers (reverse l to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Derivative and cost function\n",
    "\n",
    "From algorithm, step 3 explains the updation part through the Backpropagation. To accomplish this we need find the derivatives of errors (cost function). Cost function is the function of activation function which is sigmoid and outputs as defined below. In order to find the gradient(direction of optimal point), we have to calculate the derivative. The function for sigmoid is given in the below cell, write derivative functions for tanh and ReLU.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#### derivatives\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "#### cost function\n",
    "def cost_function(output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feed forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(a):\n",
    "    \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "    for b, w in zip(biases, weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Weight updation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_mini_batch(mini_batch, eta, biases, weights):\n",
    "    \"\"\"Update the network's weights and biases by applying\n",
    "    gradient descent using backpropagation to a single mini batch.\n",
    "    The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "    is the learning rate.\"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = backprop(x, y)\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    weights = [w-(eta/len(mini_batch))*nw\n",
    "                    for w, nw in zip(weights, nabla_w)]\n",
    "    biases = [b-(eta/len(mini_batch))*nb\n",
    "                   for b, nb in zip(biases, nabla_b)]\n",
    "    return(weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pandas import get_dummies\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_class = 10\n",
    "mdata = load_digits(n_class = n_class)\n",
    "n_feats = len(mdata.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(mdata.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = [(np.reshape(mdata.data[i], (n_feats, 1)), \n",
    "                  np.reshape(get_dummies(mdata.target).astype(int).values[i], (n_class, 1))) \n",
    "                 for i in range(len(mdata.target))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers, biases, weights = initializer([n_feats, 20, 15, 10,  n_class])\n",
    "epochs = 300\n",
    "mini_batch_size = 64\n",
    "eta = .03  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ib, iw = biases, weights # save the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    mini_batches = [\n",
    "        training_data[k:k+mini_batch_size]\n",
    "        for k in range(0, n, mini_batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Train the neural network using mini-batch stochastic\n",
    "gradient descent.  The ``training_data`` is a list of tuples\n",
    "``(x, y)`` representing the training inputs and the desired\n",
    "outputs.  The other non-optional parameters are\n",
    "self-explanatory.  If ``test_data`` is provided then the\n",
    "network will be evaluated against the test data after each\n",
    "epoch, and partial progress printed out.  This is useful for\n",
    "tracking progress, but slows things down substantially.\"\"\"\n",
    "n = len(training_data)\n",
    "for j in range(epochs):\n",
    "    # shuffing the dataset \n",
    "    random.shuffle(training_data)\n",
    "    # batches - #batches = #datapoints/batch_size\n",
    "    mini_batches = [\n",
    "        training_data[k:k+mini_batch_size]\n",
    "        for k in range(0, n, mini_batch_size)]\n",
    "    for mini_batch in mini_batches:\n",
    "        weights, biases = update_mini_batch(mini_batch, eta, biases, weights)\n",
    "        #print weights[0]\n",
    "    if j%50 == 0:\n",
    "        print(\"Epoch {0} complete\".format(j))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Given a new point, how would prediction happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feedforward(training_data[89][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = [np.argmax(feedforward(training_data[i][0])) for i in range(len(training_data))]\n",
    "actual = [np.argmax(training_data[i][1]) for i in range(len(training_data))]\n",
    "print(predicted[:5], actual[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wow, we got 20% accuracy, might not be true, calculate yourself :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post model activities (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, we have saved the initial weights. Let's paly with it. \n",
    "\n",
    "1. subrtract the initial weights with final weights for alll the layers, obtain descriptive statistics and have a look. You might get something interesting, if any post it in forum.\n",
    "\n",
    "2. plot the trained activations of each layer, have pleasure of seeing your pattern been capturing, if it happens, otherwise play with the architecture till you're convinced ;P.\n",
    "\n",
    "3. Record the trained weights for every 10th epoch while training, and make distribution plots for each of them. Why?, that's how they do, you might get the proper reason in matter of time.\n",
    "\n",
    "4. Read all the above 3 activities, get clarified if can't get the framed sentence. if you thinking of anythin about these reply to the contributers/forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iw[0]- weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iw[1] - weights[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
